{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e07905e7",
   "metadata": {
    "id": "e07905e7"
   },
   "source": [
    "## Языковое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d662181",
   "metadata": {
    "id": "9d662181"
   },
   "source": [
    "### Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f91d13b",
   "metadata": {
    "id": "0f91d13b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f55c0",
   "metadata": {
    "id": "c60f55c0"
   },
   "source": [
    "### Настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0294bc88",
   "metadata": {
    "id": "0294bc88"
   },
   "outputs": [],
   "source": [
    "PATH = './evgenyi_onegin.txt'\n",
    "\n",
    "SEQ_LENGTH = 100\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "RNN_UNITS = 1024\n",
    "EMBEDDING_DIM = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56227b2",
   "metadata": {
    "id": "b56227b2"
   },
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b022174",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b022174",
    "outputId": "2e656c58-6161-40a4-c7fd-a304c0ef1b5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143928\n",
      "Александр Сергеевич Пушкин\n",
      "\n",
      "Евгений Онегин\n",
      "Роман в стихах\n",
      "\n",
      "Не мысля гордый свет забавить,\n",
      "Вниманье дружбы возлюбя,\n",
      "Хотел бы я тебе представить\n",
      "Залог достойнее тебя,\n",
      "Достойнее души прекрасной,\n",
      "Святой исполненной мечты,\n",
      "Поэзии живой и ясной,\n",
      "Высоких дум и простоты;\n",
      "Но так и быть - рукой пристрастной\n",
      "Прими собранье пестрых глав,\n",
      "Полусмешных, полупечальных,\n",
      "Простонародных, идеальных,\n",
      "Небрежный плод моих забав,\n",
      "Бессонниц, легких вдохновений,\n",
      "Незрелых и увядших лет,\n",
      "Ума холодных наблюдений\n",
      "И сердца го\n"
     ]
    }
   ],
   "source": [
    "with open(PATH, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = '\\n'.join(line.strip() for line in text.split('\\n'))\n",
    "\n",
    "print(len(text))\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508015d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "508015d9",
    "outputId": "06250f6c-93c8-42a6-c4bf-dd48a1f5513b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143928 143928\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "char2idx = {char: i for i, char in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[char] for char in text])\n",
    "print(len(text_as_int), len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae590a5",
   "metadata": {
    "id": "1ae590a5"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(SEQ_LENGTH+1, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc4f3c3",
   "metadata": {
    "id": "7fc4f3c3"
   },
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1ec82d",
   "metadata": {
    "id": "2c1ec82d"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "                                 \n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=False,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=False,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "\n",
    "         tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=False,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "                                   \n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b36332cc",
   "metadata": {
    "id": "b36332cc"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675d2cb5",
   "metadata": {
    "id": "675d2cb5"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "model = build_model(vocab_size, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136a88d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "136a88d8",
    "outputId": "d7c124b1-8628-4e2a-f964-28625fdd3f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 149s 7s/step - loss: 4.3099\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 151s 7s/step - loss: 3.4238\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 151s 7s/step - loss: 3.1087\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 153s 7s/step - loss: 2.7958\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 153s 7s/step - loss: 2.5761\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 152s 7s/step - loss: 2.4743\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 173s 8s/step - loss: 2.3957\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 171s 8s/step - loss: 2.3229\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 167s 8s/step - loss: 2.2518\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 159s 7s/step - loss: 2.1825\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7724597f",
   "metadata": {
    "id": "7724597f"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  \n",
    "    num_generate = 512\n",
    "    input_eval = [char2idx[char] for char in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "  \n",
    "    temperature = 0.7\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "       \n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea6d1600",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea6d1600",
    "outputId": "70c58ddd-03ed-44e4-cc0d-78b9159c8fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528\n",
      "Случайный текст вебой гА воснажу\n",
      "Уной на\n",
      "Са,\n",
      "Внетой ропона; это сней тай со дой,\n",
      "Есто.\n",
      "Не\n",
      "Вон нь вист вы, та\n",
      "Вой пей нащуло\n",
      "Ва;\n",
      "Спослда праль,\n",
      "И в и дон да нет ной пы,\n",
      "Чата\n",
      "Ота, да по, нота\"bL\n",
      "Поюmf. талымох оторего ворасле орей\n",
      "2\n",
      "Пе ска пиро, у.\n",
      "Са\n",
      "Па омору\n",
      "На,\n",
      "Вой праса во бро.\n",
      "Кране.\n",
      "Нохли по, ц метойвытот простой о мотлотой сухот зажатахы\n",
      "Мащу моми нины, ой\n",
      "Грой ду зарода;\n",
      "Вы и иы стдойшамоцала бодетоли ли вя ци, щемы тали,\n",
      "Нока кохи во,\n",
      "Иело о са вотуНо\n",
      "Истойсту\n",
      "Пло бомомваку\n",
      "Прямам выхохьяне но, и Лины\n",
      "Очаногарь?\n",
      "Те,\n"
     ]
    }
   ],
   "source": [
    "gen_text = generate_text(model, start_string=\"Случайный текст \")\n",
    "print(len(gen_text), gen_text, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906ff27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
